{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install musdb numpy librosa tensorflow numpy\n",
    "# ! pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, UpSampling1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Function to convert audio to spectrogram\n",
    "def audio_to_spectrogram(audio, sr=44100, n_fft=2048, hop_length=512):\n",
    "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    spectrogram = np.abs(stft)\n",
    "    return spectrogram\n",
    "\n",
    "# Function to prepare data from the MUSDB18-HQ dataset\n",
    "def prepare_data(folder_path):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for subset in ['train', 'test']:\n",
    "        subset_path = os.path.join(folder_path, subset)\n",
    "        for track_folder in os.listdir(subset_path):\n",
    "            track_path = os.path.join(subset_path, track_folder)\n",
    "            if os.path.isdir(track_path):\n",
    "                mix_path = os.path.join(track_path, 'mixture.wav')\n",
    "                vocal_path = os.path.join(track_path, 'vocals.wav')\n",
    "\n",
    "                mix, sr = librosa.load(mix_path, sr=None, mono=False)\n",
    "                vocal, sr = librosa.load(vocal_path, sr=None, mono=False)\n",
    "                accompaniment = mix - vocal\n",
    "\n",
    "                mix_spec = np.array([audio_to_spectrogram(mix[0]), audio_to_spectrogram(mix[1])])\n",
    "                vocal_spec = np.array([audio_to_spectrogram(vocal[0]), audio_to_spectrogram(vocal[1])])\n",
    "                accompaniment_spec = np.array([audio_to_spectrogram(accompaniment[0]), audio_to_spectrogram(accompaniment[1])])\n",
    "\n",
    "                mix_spec = np.moveaxis(mix_spec, 0, -1)\n",
    "                vocal_spec = np.moveaxis(vocal_spec, 0, -1)\n",
    "                accompaniment_spec = np.moveaxis(accompaniment_spec, 0, -1)\n",
    "\n",
    "                X.append(mix_spec)\n",
    "                Y.append(np.stack((vocal_spec, accompaniment_spec), axis=-1))\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Load dataset from folder\n",
    "folder_path = 'musdb18hq'\n",
    "X_train, Y_train = prepare_data(folder_path)\n",
    "\n",
    "# Model Definition\n",
    "def wave_unet_model(input_size=(44100, 2)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv1D(24, 15, activation='relu', padding='same')(inputs)\n",
    "    pool1 = Conv1D(24, 15, strides=2, activation='relu', padding='same')(conv1)\n",
    "\n",
    "    conv2 = Conv1D(48, 15, activation='relu', padding='same')(pool1)\n",
    "    pool2 = Conv1D(48, 15, strides=2, activation='relu', padding='same')(conv2)\n",
    "\n",
    "    conv3 = Conv1D(96, 15, activation='relu', padding='same')(pool2)\n",
    "    pool3 = Conv1D(96, 15, strides=2, activation='relu', padding='same')(conv3)\n",
    "\n",
    "    conv4 = Conv1D(192, 15, activation='relu', padding='same')(pool3)\n",
    "    pool4 = Conv1D(192, 15, strides=2, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = Conv1D(384, 15, activation='relu', padding='same')(pool4)\n",
    "\n",
    "    # Decoder\n",
    "    up4 = UpSampling1D(size=2)(bottleneck)\n",
    "    up4 = concatenate([up4, conv4], axis=-1)\n",
    "    up_conv4 = Conv1D(192, 15, activation='relu', padding='same')(up4)\n",
    "\n",
    "    up3 = UpSampling1D(size=2)(up_conv4)\n",
    "    up3 = concatenate([up3, conv3], axis=-1)\n",
    "    up_conv3 = Conv1D(96, 15, activation='relu', padding='same')(up3)\n",
    "\n",
    "    up2 = UpSampling1D(size=2)(up_conv3)\n",
    "    up2 = concatenate([up2, conv2], axis=-1)\n",
    "    up_conv2 = Conv1D(48, 15, activation='relu', padding='same')(up2)\n",
    "\n",
    "    up1 = UpSampling1D(size=2)(up_conv2)\n",
    "    up1 = concatenate([up1, conv1], axis=-1)\n",
    "    up_conv1 = Conv1D(24, 15, activation='relu', padding='same')(up1)\n",
    "\n",
    "    outputs = Conv1D(2, 1, activation='sigmoid', padding='same')(up_conv1)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = wave_unet_model()\n",
    "model.summary()\n",
    "\n",
    "# Training the Model\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=8, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import pandas as pd\n",
    "\n",
    "# Function to read bad words from CSV with filtering options\n",
    "def read_bad_words(csv_file, categories=None, min_severity=None):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if categories is not None:\n",
    "        category_filter = df[['category_1', 'category_2', 'category_3']].apply(lambda x: any(item in categories for item in x if pd.notna(item)), axis=1)\n",
    "        df = df[category_filter]\n",
    "    if min_severity is not None:\n",
    "        df = df[df['severity_rating'] >= 0]\n",
    "\n",
    "\n",
    "    bad_words = set(df['text'].dropna().tolist())\n",
    "    bad_words.update(df['canonical_form_1'].dropna().tolist())\n",
    "    bad_words.update(df['canonical_form_2'].dropna().tolist())\n",
    "    bad_words.update(df['canonical_form_3'].dropna().tolist())\n",
    "    return bad_words\n",
    "\n",
    "# Function to transcribe audio to text\n",
    "def transcribe_audio(file_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio_file = sr.AudioFile(file_path)\n",
    "    with audio_file as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    return recognizer.recognize_google(audio_data)\n",
    "\n",
    "# Function to censor bad words in the audio\n",
    "def censor_audio(input_audio_path, output_audio_path, bad_words_csv, categories=None, min_severity=None, beep_duration_ms=500):\n",
    "    # Read bad words from CSV with filtering\n",
    "    bad_words = read_bad_words(bad_words_csv, categories, min_severity)\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    transcription = transcribe_audio(input_audio_path)\n",
    "    words = transcription.split()\n",
    "    \n",
    "    # Create a beep sound\n",
    "    beep = AudioSegment.silent(duration=beep_duration_ms)\n",
    "    \n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(input_audio_path)\n",
    "    \n",
    "    # Initialize the start time for each word\n",
    "    start_time = 0\n",
    "    \n",
    "    # Replace bad words with beep\n",
    "    for word in words:\n",
    "        duration = len(word) * (beep_duration_ms // 5)  # Approximate word duration\n",
    "        if word.lower() in bad_words:\n",
    "            audio = audio[:start_time] + beep + audio[start_time + duration:]\n",
    "        start_time += duration\n",
    "    \n",
    "    # Export the censored audio\n",
    "    audio.export(output_audio_path, format=\"wav\")\n",
    "\n",
    "# Example usage\n",
    "input_audio_path = \"isolated_vocal.wav\"\n",
    "output_audio_path = \"censored_vocal.wav\"\n",
    "bad_words_csv = \"profanity_en.csv\"\n",
    "categories = None  # None to include all categories\n",
    "min_severity = 2.0\n",
    "censor_audio(input_audio_path, output_audio_path, bad_words_csv, categories, min_severity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "\n",
    "predicted_vocals, sr = librosa.load('predicted_vocals.wav', sr=44100)\n",
    "\n",
    "librosa.output.write_wav(\"isolated_vocal.wav\", predicted_vocals, sr)\n",
    "\n",
    "# Censor the isolated vocal file\n",
    "input_audio_path = \"isolated_vocal.wav\"\n",
    "output_audio_path = \"censored_vocal.wav\"\n",
    "bad_words_csv = \"profanity_en.csv\"\n",
    "categories = None  # None to include all categories\n",
    "min_severity = 0\n",
    "censor_audio(input_audio_path, output_audio_path, bad_words_csv, categories, min_severity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
